<!doctype html>
<html class="reveal">
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8">

    <title>
        Bayesian Linear Regression
    </title>

    <!--
      <link rel="stylesheet" type="text/css" href="css/reset.css" />
    -->

    <!-- Reveal -->
    <link rel="stylesheet" type="text/css" href="/css/reveal.min.css" />
    <link rel="stylesheet" type="text/css" href="/css/reveal-theme.css" />
    <script type="text/javascript" src="/js/reveal.min.js"></script>
    <!-- D3js -->
    <script type="text/javascript" src="/js/d3.min.js"></script>

    <!-- AngularJS -->
    <script type="text/javascript" src="/js/underscore.min.js"></script>
    <script type="text/javascript" src="/js/jquery.min.js"></script>
    <script type="text/javascript" src="/js/angular.min.js"></script>

    <!-- Talk -->
    <link rel="stylesheet" type="text/css" href="/css/custom.css" />
    <script type="text/javascript" src="/js/talk.js"></script>
    <link rel="stylesheet" href="/css/zenburn.css" id="highlight-theme">
    <script src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>

<body ng-app="linregTalk" ng-controller="TalkCtrl" talk-slides>

<section>
    <h2>A Bayesian Approach to \(L^1\) and \(L^2\) Regularization in Machine Learning</h2>
    <h3>Colin Carroll</h3>
    <h5>Spiceworks</h5>
</section>

<section>
    <section>
        <img src="/spiceworks.png">
    </section>
    <section data-background="/max_height.jpg" data-background-size="cover"></section>
</section>

<section>
    <section>
        <h2>Linear Regression</h2>
        <ul>
            <li>
                <em>Regression</em> task: will predict a real-valued output
            </li>
            <li>
                Don't be fooled by the word <em>linear</em>: it's actually cool
            </li>
            <li>
                Quite useful on its own, and forms the basis for methods with sexier names
            </li>
        </ul>
    </section>
</section>

<section>

    <section>
        <h2>Setup</h2>
        <div class="eqnbox">
            <br>
            $$y(\mathbf{x}, \mathbf{w}) = w_0 + x_1 w_1 + \cdots + w_Dx_D$$
            <br>
        </div>
        <ul>
            <li>
                \(\mathbf{x} = (x_1, \ldots, x_D) \in \mathbb{R}^D \) are called <em>features</em>
            </li>
            <li>
                \(\mathbf{w} = (w_0, \ldots, w_D) \in \mathbb{R}^D \) are called <em>weights</em>
            </li>
        </ul>
    </section>

    <section>
        <h2>Advanced Setup</h2>

        <div class="eqnbox">
            $$y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{j=1}^n w_j \phi_j(\mathbf{x})$$
        </div>

        <ul>
            <li>
                \(\phi_j: \mathbb{R}^D \to \mathbb{R}\) is called a <em>basis function</em>
            </li>
            <li>
                \(\phi_0(\mathbf{x}) := 1\), so that
            </li>
        </ul>

        <div style="font-size: 36pt;">
            $$ y = \phi(\mathbf{x}) \cdot \mathbf{w}$$
        </div>
    </section>

    <section>
        <plot-line point-data="polynomial" xkey="x" ykey="y"></plot-line>
        $$ \phi(x) = x^2 $$
    </section>

    <section>
        <plot-line point-data="sigmoid" xkey="x" ykey="y"></plot-line>
        $$ \phi(x) = \frac{1}{1 + e^{-5x}} $$
    </section>
    <section>
        <plot-line point-data="gaussian" xkey="x" ykey="y"></plot-line>
        $$ \phi(x) = e^{\frac{x^2}{0.5^2}} $$
    </section>

</section>

<section>

    <section>
        <h2>Loss Function</h2>

        Given data \(\{(y_1, \mathbf{x}_1),\ldots,(y_D, \mathbf{x}_D)\}\), find weights
        \(\mathbf{w} = (w_1, \ldots, w_n)\) to minimize
        $$
        loss(\mathbf{w}) = \sum_{j=1}^D (y_j - \mathbf{w} \cdot \phi(\mathbf{x}_j))^2
        $$
    </section>

    <section>
        <h2>Linear algebra to the rescue</h2>
        $$
        X = (\mathbf{x}_1, \ldots, \mathbf{x}_D), \mathbf{y} = (y_1, \ldots, y_D)^T
        $$
        Then
        $$\mathbf{w} = (X^TX)^{-1}X^T\mathbf{y}$$
        minimizes $$\|X\mathbf{w} - \mathbf{y}\|_2^2$$
    </section>

    <section>
        <h2>Calculus to the rescue</h2>
        $$ \nabla_{\mathbf{w}}loss = \sum_{j=1}^D (\mathbf{w} \cdot \phi(x_j) - y_j) \phi(x_j) $$
        So iterate the following step until \(\mathbf{w}\) converges:
        $$\mathbf{w} = \mathbf{w} - \alpha \nabla_{\mathbf{w}}loss$$
    </section>
</section>

<section>

    <section>
        <h2>A Naive Approach</h2>

        <ul>
            <li>
                Create a bunch of basis functions
            </li>
            <li>
                Fit model with very low error
            </li>
            <li>
                Realize model overfits data
            </li>
        </ul>
    </section>
    <section>
        <plot-model model-data="modelData"></plot-model>
        {{modelData}}
    </section>
</section>

<section>

    <h2>A Less Naive Approach</h2>

    <ul>
        <li>
            Split data into training and testing sets
        </li>
        <li>
            Create a bunch of basis functions
        </li>
        <li>
            Fit model on subset of basis functions and the training data
        </li>
        <li>
            Choose model with smallest testing error
        </li>
    </ul>

</section>

<section>

    <h2>A Typical Approach</h2>

    <ul>
        <li>
            Split data into training and testing sets
        </li>
        <li>
            Create a bunch of basis functions
        </li>
        <li>
            Choose weights to minimize either squared training error or
            $$ \sum_{j=1}^D (y_j - \mathbf{w} \cdot \phi(\mathbf{x}_j))^2 + C\|\mathbf{w}\|_2^2 $$
            or
            $$ \sum_{j=1}^D (y_j - \mathbf{w} \cdot \phi(\mathbf{x}_j))^2 + C\|\mathbf{w}\|_1 $$
        </li>
    </ul>

</section>

<section>

    <h2>A Typical Approach</h2>
    <pre><code>
        errors = []
        for penalty in (None, 'l1', 'l2'):
        for constant in (0.001, 0.03, 0.1, 0.3, 1):
        model = linear_regression.fit(training_data, penalty, constant)
        errors.append(
        sum(
        model.fit(testing_data.features) - testing_data.labels
        ) ** 2
        )
    </code></pre>
</section>

<section>

    <section>

        <h2>Motivating Loss Function</h2>
        <div class="eqnbox">
            <br>
            $$y(\mathbf{x}, \mathbf{w}) = \mathbf{w} \cdot \mathbf{x} + \mathscr{N}(0, \sigma^2)$$
            <br>
        </div>
        The <em>likelihood</em> is given by
        <div class="eqnbox">
            <br>
            $$p(y | \mathbf{x}, \mathbf{w}) \propto \exp{\left(-\frac{(y - \mathbf{w} \cdot \mathbf{x})^2}{2 \sigma^2}\right)}$$
            <br>
        </div>
    </section>

    <section>

        <h2>Normal Distribution</h2>

        <ul>
            <li>
                \(\mathscr{N}(x | \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}\)
            </li>
            <li>
                \(\mu\) is the mean, \(\sigma\) is the standard deviation (these are theorems, not definitions)
            </li>
            <li>
                \(\mathscr{N}(\mu, \sigma)\) is a random variable with mean \(\mu\), variance \(\sigma^2\)
            </li>
        </ul>
    </section>

    <section>
        <h2>Why the normal distribution?</h2>
        Something something something central limit theorem. Analytically tractable.  Gives least squares.
    </section>

</section>

<section ng-click="cycleData()" ng-blur="restoreDefaults()">
    <plot-points point-data="data"></plot-points>
    <div>
        {{parameters.trainingPoints}} points drawn from
        <br>
        <span mathjax-bind="latexGenFunc"></span>
    </div>
</section>

<section>
    <h2>Motivating Loss Function</h2>
    $$\mathscr{D} = \{(y_1, \mathbf{x}_1), \ldots, (y_N, \mathbf{x}_N)\}$$
    <div class="eqnbox">
        $$
        \begin{align}
        p(\mathscr{D} | \mathbf{w}) &= \prod_{j=1}^N p(y_j | \mathbf{x}_j, \mathbf{w}) \\
        & \propto \exp{\sum_{j=1}^N -\frac{(y_j - \mathbf{w} \cdot \mathbf{x_j})^2}{2 \sigma^2}}
        \end{align}
        $$
    </div>
</section>

<section>
    <h2>Motivating Loss Function</h2>
    Maximizing $$\exp{\sum_{j=1}^N -\frac{(y_j - \mathbf{w} \cdot \mathbf{x_j})^2}{2 \sigma^2}}$$
    is equivalent to mimimizing
    $$ \sum_{j=1}^N (y_j - \mathbf{w} \cdot \mathbf{x_j})^2 $$
</section>

<section>

    <section>
        <h2>More priors</h2>
        What if we had some <em>prior expectations</em> about the weights of the model?
        <ul>
            <li>
                If we expect the weights to all be fairly small, we might write \(\mathbf{w} \sim \mathscr{N}(0, \tau)\),
                where \(\tau\) is a measure of how small we expect the weights to be.
            </li>
            <li>
                We might also write \(\mathbf{w} \sim \text{Laplace}(0, \tau)\),
                where \(\tau\) is again again a scale parameter.
            </li>
        </ul>
    </section>

    <section>
        <h2>Laplace Distribution</h2>
        <ul>
            <li>
                \(\text{Laplace}(x | \mu, \tau) = \frac{1}{2\tau}e^{-\frac{|x - \mu|}{\tau}}\)
            </li>
            <li>
                \(\mu\) is the mean, \(\tau\sqrt{2}\) is the standard deviation
            </li>
        </ul>
    </section>

</section>

<section>
    <h2>More priors</h2>
    Using Bayes' theorem
    $$
    p(\mathbf{w} | \mathscr{D}, \tau) \propto p(\mathscr{D} | \mathbf{w}, \tau) p(\mathbf{w}| \tau)
    $$
    <br>
    $$
    \begin{align}
    -\log{\left(p(\mathbf{w} | \mathscr{D}, \tau)\right)} &\propto -\log{\left(p(\mathscr{D} | \mathbf{w}, \tau)\right)} - \log{\left(p(\mathbf{w}, \tau)\right)} \\
    &= \frac{\sum (y_j - \mathbf{w} \cdot \mathbf{x}_j)^2}{2 \sigma^2} - \log{\left(p(\mathbf{w}, \tau)\right)}
    \end{align}
    $$
</section>

<section>
    <h2>Ridge Regression</h2>
    $$
    p(\mathbf{w}, \tau) \propto \exp{\left(-\frac{\|\mathbf{w}\|_2^2}{2\tau^2}\right)}
    $$
    $$
    loss(w) = \|\mathbf{y} - \mathbf{X}\mathbf{w}^T\|^2 + \left(\frac{\sigma}{\tau}\right)^2\|\mathbf{w}\|_2^2
    $$
</section>
<section>
    <h2>Lasso Regression</h2>
    $$
    p(\mathbf{w}, \tau) \propto \exp{\left(-\frac{\|\mathbf{w}\|_1}{\tau}\right)}
    $$
    $$
    loss(w) = \|\mathbf{y} - \mathbf{X}\mathbf{w}^T\|^2 + \frac{2\sigma^2}{\tau}\|\mathbf{w}\|_1
    $$
</section>


</body>

</html>
